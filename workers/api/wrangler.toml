name = "robot-scraping-core"
main = "src/index.ts"
compatibility_date = "2024-10-01"
compatibility_flags = ["nodejs_compat"]

[browser]
binding = "MYBROWSER"

[[d1_databases]]
binding = "DB"
database_name = "robot-db"
database_id = "8c56033f-b93e-4f8c-b836-7b2a291db59c"

[[r2_buckets]]
binding = "BUCKET"
bucket_name = "robot-snapshots"

[[queues.producers]]
queue = "robot-tasks"
binding = "TASK_QUEUE"

[[queues.consumers]]
queue = "robot-tasks"
max_batch_size = 1
max_batch_timeout = 60

[triggers]
crons = ["* * * * *"]

[vars]
AI_PROVIDER = "openai"
OPENAI_MODEL = "gpt-4o-mini"
ANTHROPIC_MODEL = "claude-3-haiku-20240307"
MAX_CONTENT_CHARS = "20000"
BROWSER_TIMEOUT_MS = "15000"
CORS_ORIGIN = "https://robotscraping.com"
DEFAULT_SCREENSHOT = "false"
STORE_CONTENT = "true"
ALLOW_ANON = "false"
CACHE_ENABLED = "true"
CACHE_TTL_MS = "900000"
PROXY_GRID_ENABLED = "false"
PROXY_GRID_BASE_URL = "http://google.savedimage.com"
PROXY_GRID_ALLOWLIST = ""
PROXY_GRID_FORCE = "false"
# Security: WEBHOOK_SECRET must be set via secret binding (wrangler secret put WEBHOOK_SECRET)
# Never commit actual secrets to git
RATE_LIMIT_ENABLED = "true"
RATE_LIMIT_REQUESTS = "60"
RATE_LIMIT_WINDOW_MS = "60000"
MAX_REQUEST_SIZE_MB = "10"
ENABLE_RATE_LIMIT_HEADERS = "true"
