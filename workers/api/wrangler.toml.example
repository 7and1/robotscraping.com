name = "robot-scraping-core"
main = "src/index.ts"
compatibility_date = "2024-10-01"
compatibility_flags = ["nodejs_compat"]

# Browser binding
[browser]
binding = "MYBROWSER"

# D1 Database - set database_id via environment variable or update after creation
[[d1_databases]]
binding = "DB"
database_name = "robot-db"
database_id = "YOUR_D1_DATABASE_ID"  # Replace with actual ID from: npx wrangler d1 create robot-db

# R2 Bucket
[[r2_buckets]]
binding = "BUCKET"
bucket_name = "robot-snapshots"

# Queue
[[queues.producers]]
queue = "robot-tasks"
binding = "TASK_QUEUE"

[[queues.consumers]]
queue = "robot-tasks"
max_batch_size = 1
max_batch_timeout = 60

# Cron trigger
[triggers]
crons = ["* * * * *"]

# Environment variables (can also be set via Cloudflare dashboard)
[vars]
AI_PROVIDER = "openai"
OPENAI_MODEL = "gpt-4o-mini"
ANTHROPIC_MODEL = "claude-3-haiku-20240307"
MAX_CONTENT_CHARS = "20000"
BROWSER_TIMEOUT_MS = "15000"
CORS_ORIGIN = "*"
DEFAULT_SCREENSHOT = "false"
STORE_CONTENT = "true"
WEBHOOK_SECRET = "change-me-in-production"
ALLOW_ANON = "false"
CACHE_ENABLED = "true"
CACHE_TTL_MS = "900000"
PROXY_GRID_ENABLED = "false"
PROXY_GRID_BASE_URL = "http://google.savedimage.com"
PROXY_GRID_ALLOWLIST = ""
PROXY_GRID_FORCE = "false"
RATE_LIMIT_ENABLED = "true"
RATE_LIMIT_REQUESTS = "60"
RATE_LIMIT_WINDOW_MS = "60000"
MAX_REQUEST_SIZE_MB = "10"
ENABLE_RATE_LIMIT_HEADERS = "true"
