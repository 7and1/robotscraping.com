name = "robot-scraping-core"
main = "src/index.ts"
compatibility_date = "2024-10-01"
compatibility_flags = ["nodejs_compat"]

# Account ID is provided via environment variable CLOUDFLARE_ACCOUNT_ID
# Never commit account_id to prevent exposure in public repos

[browser]
binding = "MYBROWSER"

[[d1_databases]]
binding = "DB"
database_name = "robot-db"
database_id = "YOUR_DATABASE_ID_HERE"

[[r2_buckets]]
binding = "BUCKET"
bucket_name = "robot-snapshots"

[[queues.producers]]
queue = "robot-tasks"
binding = "TASK_QUEUE"

[[queues.consumers]]
queue = "robot-tasks"
max_batch_size = 1
max_batch_timeout = 60

[triggers]
crons = ["* * * * *"]

[vars]
AI_PROVIDER = "openrouter"
OPENROUTER_MODEL = "meta-llama/llama-3.3-70b-instruct:free"
OPENROUTER_FALLBACK_MODEL_1 = "deepseek/deepseek-r1:free"
OPENROUTER_FALLBACK_MODEL_2 = "google/gemini-3-flash-preview"
# Legacy: OpenAI/Anthropic
# OPENAI_MODEL = "gpt-4o-mini"
# ANTHROPIC_MODEL = "claude-3-haiku-20240307"
MAX_CONTENT_CHARS = "20000"
BROWSER_TIMEOUT_MS = "15000"
CORS_ORIGIN = "https://robotscraping.com"
DEFAULT_SCREENSHOT = "false"
STORE_CONTENT = "true"
ALLOW_ANON = "true"
CACHE_ENABLED = "true"
CACHE_TTL_MS = "900000"
PROXY_GRID_ENABLED = "false"
PROXY_GRID_BASE_URL = "http://google.savedimage.com"
PROXY_GRID_ALLOWLIST = ""
PROXY_GRID_FORCE = "false"
# Security: WEBHOOK_SECRET must be set via secret binding (wrangler secret put WEBHOOK_SECRET)
# Never commit actual secrets to git
RATE_LIMIT_ENABLED = "true"
RATE_LIMIT_REQUESTS = "60"
RATE_LIMIT_WINDOW_MS = "60000"
MAX_REQUEST_SIZE_MB = "10"
ENABLE_RATE_LIMIT_HEADERS = "true"
SESSION_COOKIE_NAME = "rs_session"
SESSION_COOKIE_DOMAIN = ".robotscraping.com"
SESSION_COOKIE_SECURE = "true"
SESSION_TTL_HOURS = "168"
AUTH_SUCCESS_REDIRECT = "https://robotscraping.com/login"
GITHUB_SCOPES = "read:user,user:email"
# GitHub OAuth credentials should be set via secrets:
# wrangler secret put GITHUB_CLIENT_ID
# wrangler secret put GITHUB_CLIENT_SECRET
